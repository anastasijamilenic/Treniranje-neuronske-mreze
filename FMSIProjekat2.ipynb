{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "b4e0fdbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "81e78ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CovertypeClassification(nn.Module):\n",
    "  def __init__(self, input_size, hidden_size, output_size):\n",
    "    super().__init__()\n",
    "    self.layer1 = nn.Linear(54, 128)\n",
    "    self.act1 = nn.ReLU()\n",
    "    self.drop1 = nn.Dropout(0.2)\n",
    "    self.layer2 = nn.Linear(128, 128)\n",
    "    self.act2 = nn.ReLU()\n",
    "    self.drop2 = nn.Dropout(0.2)\n",
    "    self.layer3 = nn.Linear(128, 64)\n",
    "    self.act3 = nn.ReLU()\n",
    "    self.output = nn.Linear(64, 7)\n",
    "    self.softmax = nn.Softmax(dim = 1)\n",
    "    \n",
    "   \n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.act1(self.layer1(x))\n",
    "    x = self.drop1(x)\n",
    "    x = self.act2(self.layer2(x))\n",
    "    x = self.drop2(x)\n",
    "    x = self.act3(self.layer3(x))\n",
    "    x = self.output(x)\n",
    "    x = self.softmax(x)\n",
    "    return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "55dd0942",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "bf045883",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_covtype\n",
    "\n",
    "covertype = fetch_covtype()\n",
    "X = covertype.data\n",
    "y = covertype.target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "4287d57e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "bdd42e94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "8b855c9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 1, 2, 3, 4, 5, 6])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "X_train_t = torch.tensor(X_train).float()\n",
    "X_test_t = torch.tensor(X_test).float()\n",
    "y_train_t = torch.tensor(y_train).long().reshape(-1)\n",
    "y_train_t = y_train_t -1\n",
    "y_test_t = torch.tensor(y_test).long().reshape(-1)\n",
    "y_test_t = y_test_t - 1\n",
    "unique_labels = y_train_t.unique()\n",
    "print(unique_labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "e38fa3d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([464809, 54])\n",
      "torch.Size([464809])\n"
     ]
    }
   ],
   "source": [
    "print(X_train_t.shape)\n",
    "print(y_train_t.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "bff2ec36",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(X_train_t, y_train_t)\n",
    "test_dataset = TensorDataset(X_test_t, y_test_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "a9d6a0ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size = 32, shuffle = True)\n",
    "test_loader = DataLoader(test_dataset, batch_size = 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "d00067e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\") \n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "dfe9cbe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = CovertypeClassification(54, 128, 7).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "fd2baaf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "a31001ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(net, loss_fn, test_loader):\n",
    "    total_loss = 0\n",
    "    net.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for x,y in test_loader:\n",
    "            x = x.to(device) \n",
    "            y = y.to(device)\n",
    "            preds = net(x)\n",
    "            preds_multiclass = np.argmax(preds, axis = 1)\n",
    "            loss = loss_fn(preds, y)\n",
    "            total_loss += loss.item()\n",
    "            precision = precision_score(y, preds_multiclass, average = 'weighted', zero_division = 0)\n",
    "            recall = recall_score(y, preds_multiclass, average = 'weighted', zero_division = 0)\n",
    "            \n",
    "           \n",
    "            \n",
    "    print(total_loss / len(test_loader))  \n",
    "    print(precision)\n",
    "    print(recall)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "08f1d74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(net.parameters(), lr = 1e-3, weight_decay = 1e-5)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "76544cef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.128254163600012\n",
      "0.0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "test_model(net, loss_fn, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "a67f5e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "52c13bd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running epoch: 0\n",
      "Running epoch: 5\n",
      "Running epoch: 10\n",
      "Running epoch: 15\n",
      "Running epoch: 20\n",
      "Running epoch: 25\n",
      "Running epoch: 30\n",
      "Running epoch: 35\n",
      "Running epoch: 40\n",
      "Running epoch: 45\n"
     ]
    }
   ],
   "source": [
    "net.train() \n",
    "for i in range(EPOCHS):\n",
    "    if i % 5 == 0:\n",
    "        print(f\"Running epoch: {i}\")\n",
    "    \n",
    "    \n",
    "    for x, y in train_loader:\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        preds = net(x)\n",
    "        loss = loss_fn(preds, y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "9766f3d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5255712237258314\n",
      "0.5969696969696969\n",
      "0.7272727272727273\n"
     ]
    }
   ],
   "source": [
    "test_model(net, loss_fn, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "59eb2ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_loss = float('inf')\n",
    "best_epoch = 0\n",
    "tolerance = 0\n",
    "max_tolerance = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "0862a63e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_t, y_train_t, test_size=0.2, random_state=42)\n",
    "\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "val_dataset = TensorDataset(X_val, y_val)\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size = 32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size = 32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "2396d4eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss=1.5780, Val Loss=1.5940\n",
      "Epoch 2: Train Loss=1.5842, Val Loss=1.4514\n",
      "Epoch 3: Train Loss=1.5808, Val Loss=1.8818\n",
      "Epoch 4: Train Loss=1.5797, Val Loss=1.4608\n",
      "Epoch 5: Train Loss=1.5763, Val Loss=1.4514\n",
      "Epoch 6: Train Loss=1.5764, Val Loss=1.1660\n",
      "Epoch 7: Train Loss=1.5833, Val Loss=1.8762\n",
      "Epoch 8: Train Loss=1.5771, Val Loss=1.3097\n",
      "Epoch 9: Train Loss=1.5781, Val Loss=1.4532\n",
      "Early stopping at epoch 9\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss = 0.0\n",
    "    val_loss = 0.0\n",
    "    net.train()\n",
    "    for x, y in train_loader:\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        preds = net(x)\n",
    "        loss = loss_fn(preds, y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        \n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        for x_val, y_val in val_loader:\n",
    "            preds_val = net(x_val)\n",
    "            loss_val = loss_fn(preds_val, y_val)\n",
    "            val_loss += loss.item()    \n",
    "            \n",
    " \n",
    "    train_loss /= len(train_loader)\n",
    "    val_loss /= len(val_loader)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}: Train Loss={train_loss:.4f}, Val Loss={val_loss:.4f}\")\n",
    "\n",
    "    \n",
    "    if val_loss < best_loss:\n",
    "        best_loss = val_loss\n",
    "        best_epoch = epoch\n",
    "        tolerance = 0\n",
    "    else:\n",
    "        tolerance += 1\n",
    "\n",
    "   \n",
    "    if tolerance > max_tolerance:\n",
    "        print(f\"Early stopping at epoch {epoch+1}\")\n",
    "        break\n",
    "\n",
    "\n",
    "         \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "65289f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = [0.001, 0.01, 0.1]\n",
    "batch_sizes = [32, 64, 128]\n",
    "size_of_hidden_layers = [64, 128, 256]\n",
    "\n",
    "best_precision = 0.0\n",
    "best_model = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "8c922b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_loss = float('inf')\n",
    "best_epoch = 0\n",
    "tolerance = 0\n",
    "max_tolerance = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "05376908",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.25\n",
      "Epoch 1: Train Loss=1.5740, Val Loss=1.8795\n",
      "0.25\n",
      "Epoch 2: Train Loss=1.5747, Val Loss=1.3134\n",
      "0.25\n",
      "Epoch 3: Train Loss=1.5740, Val Loss=1.8794\n",
      "0.25\n",
      "Epoch 4: Train Loss=1.5737, Val Loss=1.7067\n",
      "0.25\n",
      "Epoch 5: Train Loss=1.5741, Val Loss=1.3301\n",
      "Early stopping at epoch 5\n",
      "0.5294117647058824\n",
      "Epoch 1: Train Loss=1.5746, Val Loss=1.5633\n",
      "0.5294117647058824\n",
      "Epoch 2: Train Loss=1.5743, Val Loss=1.5647\n",
      "0.5294117647058824\n",
      "Epoch 3: Train Loss=1.5747, Val Loss=1.4510\n",
      "0.5294117647058824\n",
      "Epoch 4: Train Loss=1.5751, Val Loss=1.5951\n",
      "0.5294117647058824\n",
      "Epoch 5: Train Loss=1.5742, Val Loss=1.6464\n",
      "0.5294117647058824\n",
      "Epoch 6: Train Loss=1.5736, Val Loss=1.4513\n",
      "Early stopping at epoch 6\n",
      "0.5294117647058824\n",
      "Epoch 1: Train Loss=1.5746, Val Loss=1.5942\n",
      "0.5294117647058824\n",
      "Epoch 2: Train Loss=1.5737, Val Loss=1.5945\n",
      "0.5294117647058824\n",
      "Epoch 3: Train Loss=1.5741, Val Loss=1.4513\n",
      "0.5294117647058824\n",
      "Epoch 4: Train Loss=1.5736, Val Loss=1.5921\n",
      "0.5294117647058824\n",
      "Epoch 5: Train Loss=1.5744, Val Loss=1.3085\n",
      "0.5294117647058824\n",
      "Epoch 6: Train Loss=1.5749, Val Loss=1.5562\n",
      "0.5294117647058824\n",
      "Epoch 7: Train Loss=1.5740, Val Loss=1.6786\n",
      "0.5294117647058824\n",
      "Epoch 8: Train Loss=1.5739, Val Loss=1.8795\n",
      "Early stopping at epoch 8\n",
      "0.25\n",
      "Epoch 1: Train Loss=1.5745, Val Loss=1.8789\n",
      "0.25\n",
      "Epoch 2: Train Loss=1.5738, Val Loss=1.7369\n",
      "0.25\n",
      "Epoch 3: Train Loss=1.5742, Val Loss=1.6165\n",
      "0.25\n",
      "Epoch 4: Train Loss=1.5738, Val Loss=1.3513\n",
      "0.25\n",
      "Epoch 5: Train Loss=1.5737, Val Loss=1.9237\n",
      "0.25\n",
      "Epoch 6: Train Loss=1.5737, Val Loss=1.8800\n",
      "0.25\n",
      "Epoch 7: Train Loss=1.5742, Val Loss=1.7580\n",
      "Early stopping at epoch 7\n",
      "0.5294117647058824\n",
      "Epoch 1: Train Loss=1.5740, Val Loss=1.7171\n",
      "0.5294117647058824\n",
      "Epoch 2: Train Loss=1.5738, Val Loss=1.5939\n",
      "0.5294117647058824\n",
      "Epoch 3: Train Loss=1.5739, Val Loss=1.4513\n",
      "0.5294117647058824\n",
      "Epoch 4: Train Loss=1.5736, Val Loss=1.6252\n",
      "0.5294117647058824\n",
      "Epoch 5: Train Loss=1.5740, Val Loss=1.3087\n",
      "0.5294117647058824\n",
      "Epoch 6: Train Loss=1.5740, Val Loss=1.5672\n",
      "0.5294117647058824\n",
      "Epoch 7: Train Loss=1.5739, Val Loss=1.4508\n",
      "0.5294117647058824\n",
      "Epoch 8: Train Loss=1.5746, Val Loss=1.7371\n",
      "Early stopping at epoch 8\n",
      "0.5294117647058824\n",
      "Epoch 1: Train Loss=1.5742, Val Loss=1.5278\n",
      "0.5294117647058824\n",
      "Epoch 2: Train Loss=1.5733, Val Loss=1.6880\n",
      "0.5294117647058824\n",
      "Epoch 3: Train Loss=1.5746, Val Loss=1.5938\n",
      "0.5294117647058824\n",
      "Epoch 4: Train Loss=1.5738, Val Loss=1.4513\n",
      "0.5294117647058824\n",
      "Epoch 5: Train Loss=1.5745, Val Loss=1.7994\n",
      "0.5294117647058824\n",
      "Epoch 6: Train Loss=1.5744, Val Loss=1.7587\n",
      "0.5294117647058824\n",
      "Epoch 7: Train Loss=1.5737, Val Loss=1.6200\n",
      "Early stopping at epoch 7\n",
      "0.25\n",
      "Epoch 1: Train Loss=1.5739, Val Loss=1.4454\n",
      "0.25\n",
      "Epoch 2: Train Loss=1.5748, Val Loss=1.5939\n",
      "0.25\n",
      "Epoch 3: Train Loss=1.5741, Val Loss=1.5002\n",
      "0.25\n",
      "Epoch 4: Train Loss=1.5748, Val Loss=1.4408\n",
      "0.25\n",
      "Epoch 5: Train Loss=1.5736, Val Loss=1.3388\n",
      "0.25\n",
      "Epoch 6: Train Loss=1.5742, Val Loss=1.7811\n",
      "0.25\n",
      "Epoch 7: Train Loss=1.5735, Val Loss=1.7297\n",
      "0.25\n",
      "Epoch 8: Train Loss=1.5741, Val Loss=1.4927\n",
      "Early stopping at epoch 8\n",
      "0.5294117647058824\n",
      "Epoch 1: Train Loss=1.5742, Val Loss=1.4512\n",
      "0.5294117647058824\n",
      "Epoch 2: Train Loss=1.5746, Val Loss=1.4522\n",
      "0.5294117647058824\n",
      "Epoch 3: Train Loss=1.5735, Val Loss=1.5945\n",
      "0.5294117647058824\n",
      "Epoch 4: Train Loss=1.5741, Val Loss=1.5945\n",
      "Early stopping at epoch 4\n",
      "0.5294117647058824\n",
      "Epoch 1: Train Loss=1.5745, Val Loss=1.7366\n",
      "0.5294117647058824\n",
      "Epoch 2: Train Loss=1.5742, Val Loss=1.7367\n",
      "0.5294117647058824\n",
      "Epoch 3: Train Loss=1.5737, Val Loss=1.8796\n",
      "0.5294117647058824\n",
      "Epoch 4: Train Loss=1.5741, Val Loss=1.5971\n",
      "0.5294117647058824\n",
      "Epoch 5: Train Loss=1.5740, Val Loss=1.3535\n",
      "0.5294117647058824\n",
      "Epoch 6: Train Loss=1.5744, Val Loss=1.7365\n",
      "0.5294117647058824\n",
      "Epoch 7: Train Loss=1.5738, Val Loss=1.7369\n",
      "0.5294117647058824\n",
      "Epoch 8: Train Loss=1.5745, Val Loss=1.3086\n",
      "0.5294117647058824\n",
      "Epoch 9: Train Loss=1.5738, Val Loss=1.5826\n",
      "0.5294117647058824\n",
      "Epoch 10: Train Loss=1.5741, Val Loss=1.6227\n",
      "0.5294117647058824\n",
      "Epoch 11: Train Loss=1.5738, Val Loss=1.4509\n",
      "Early stopping at epoch 11\n",
      "0.25\n",
      "Epoch 1: Train Loss=1.5740, Val Loss=1.7367\n",
      "0.25\n",
      "Epoch 2: Train Loss=1.5739, Val Loss=1.7188\n",
      "0.25\n",
      "Epoch 3: Train Loss=1.5734, Val Loss=1.5940\n",
      "0.25\n",
      "Epoch 4: Train Loss=1.5741, Val Loss=1.7460\n",
      "0.25\n",
      "Epoch 5: Train Loss=1.5742, Val Loss=1.3086\n",
      "0.25\n",
      "Epoch 6: Train Loss=1.5743, Val Loss=1.5487\n",
      "0.25\n",
      "Epoch 7: Train Loss=1.5740, Val Loss=1.4512\n",
      "0.25\n",
      "Epoch 8: Train Loss=1.5738, Val Loss=1.3659\n",
      "Early stopping at epoch 8\n",
      "0.5294117647058824\n",
      "Epoch 1: Train Loss=1.5747, Val Loss=1.8404\n",
      "0.5294117647058824\n",
      "Epoch 2: Train Loss=1.5734, Val Loss=1.6488\n",
      "0.5294117647058824\n",
      "Epoch 3: Train Loss=1.5745, Val Loss=1.4512\n",
      "0.5294117647058824\n",
      "Epoch 4: Train Loss=1.5743, Val Loss=1.4499\n",
      "0.5294117647058824\n",
      "Epoch 5: Train Loss=1.5745, Val Loss=1.7370\n",
      "0.5294117647058824\n",
      "Epoch 6: Train Loss=1.5747, Val Loss=1.5988\n",
      "0.5294117647058824\n",
      "Epoch 7: Train Loss=1.5741, Val Loss=1.8790\n",
      "Early stopping at epoch 7\n",
      "0.5294117647058824\n",
      "Epoch 1: Train Loss=1.5745, Val Loss=1.5940\n",
      "0.5294117647058824\n",
      "Epoch 2: Train Loss=1.5736, Val Loss=1.4514\n",
      "0.5294117647058824\n",
      "Epoch 3: Train Loss=1.5745, Val Loss=1.5963\n",
      "0.5294117647058824\n",
      "Epoch 4: Train Loss=1.5740, Val Loss=1.4513\n",
      "0.5294117647058824\n",
      "Epoch 5: Train Loss=1.5742, Val Loss=1.3283\n",
      "0.5294117647058824\n",
      "Epoch 6: Train Loss=1.5739, Val Loss=1.5941\n",
      "0.5294117647058824\n",
      "Epoch 7: Train Loss=1.5738, Val Loss=1.3085\n",
      "0.5294117647058824\n",
      "Epoch 8: Train Loss=1.5737, Val Loss=1.6356\n",
      "0.5294117647058824\n",
      "Epoch 9: Train Loss=1.5737, Val Loss=1.5487\n",
      "0.5294117647058824\n",
      "Epoch 10: Train Loss=1.5746, Val Loss=1.2694\n",
      "0.5294117647058824\n",
      "Epoch 11: Train Loss=1.5742, Val Loss=1.5742\n",
      "0.5294117647058824\n",
      "Epoch 12: Train Loss=1.5739, Val Loss=1.3091\n",
      "0.5294117647058824\n",
      "Epoch 13: Train Loss=1.5739, Val Loss=1.5645\n",
      "Early stopping at epoch 13\n",
      "0.25\n",
      "Epoch 1: Train Loss=1.5741, Val Loss=1.7078\n",
      "0.25\n",
      "Epoch 2: Train Loss=1.5741, Val Loss=1.4512\n",
      "0.25\n",
      "Epoch 3: Train Loss=1.5744, Val Loss=1.3766\n",
      "0.25\n",
      "Epoch 4: Train Loss=1.5745, Val Loss=1.7365\n",
      "0.25\n",
      "Epoch 5: Train Loss=1.5739, Val Loss=1.7084\n",
      "0.25\n",
      "Epoch 6: Train Loss=1.5748, Val Loss=1.4519\n",
      "Early stopping at epoch 6\n",
      "0.5294117647058824\n",
      "Epoch 1: Train Loss=1.5743, Val Loss=1.6972\n",
      "0.5294117647058824\n",
      "Epoch 2: Train Loss=1.5745, Val Loss=1.3080\n",
      "0.5294117647058824\n",
      "Epoch 3: Train Loss=1.5734, Val Loss=1.4513\n",
      "0.5294117647058824\n",
      "Epoch 4: Train Loss=1.5749, Val Loss=1.7704\n",
      "0.5294117647058824\n",
      "Epoch 5: Train Loss=1.5742, Val Loss=1.4074\n",
      "Early stopping at epoch 5\n",
      "0.5294117647058824\n",
      "Epoch 1: Train Loss=1.5738, Val Loss=1.5944\n",
      "0.5294117647058824\n",
      "Epoch 2: Train Loss=1.5743, Val Loss=1.3085\n",
      "0.5294117647058824\n",
      "Epoch 3: Train Loss=1.5740, Val Loss=1.5658\n",
      "0.5294117647058824\n",
      "Epoch 4: Train Loss=1.5742, Val Loss=1.3092\n",
      "0.5294117647058824\n",
      "Epoch 5: Train Loss=1.5742, Val Loss=1.4938\n",
      "Early stopping at epoch 5\n",
      "0.25\n",
      "Epoch 1: Train Loss=1.5735, Val Loss=1.5943\n",
      "0.25\n",
      "Epoch 2: Train Loss=1.5737, Val Loss=1.7807\n",
      "0.25\n",
      "Epoch 3: Train Loss=1.5741, Val Loss=1.5936\n",
      "0.25\n",
      "Epoch 4: Train Loss=1.5743, Val Loss=1.1663\n",
      "0.25\n",
      "Epoch 5: Train Loss=1.5741, Val Loss=1.4553\n",
      "0.25\n",
      "Epoch 6: Train Loss=1.5749, Val Loss=1.5944\n",
      "0.25\n",
      "Epoch 7: Train Loss=1.5740, Val Loss=1.5943\n",
      "Early stopping at epoch 7\n",
      "0.5294117647058824\n",
      "Epoch 1: Train Loss=1.5741, Val Loss=1.7086\n",
      "0.5294117647058824\n",
      "Epoch 2: Train Loss=1.5746, Val Loss=1.6818\n",
      "0.5294117647058824\n",
      "Epoch 3: Train Loss=1.5741, Val Loss=1.5451\n",
      "0.5294117647058824\n",
      "Epoch 4: Train Loss=1.5738, Val Loss=1.3085\n",
      "0.5294117647058824\n",
      "Epoch 5: Train Loss=1.5740, Val Loss=1.5941\n",
      "0.5294117647058824\n",
      "Epoch 6: Train Loss=1.5741, Val Loss=1.5940\n",
      "0.5294117647058824\n",
      "Epoch 7: Train Loss=1.5738, Val Loss=1.7299\n",
      "Early stopping at epoch 7\n",
      "0.5294117647058824\n",
      "Epoch 1: Train Loss=1.5734, Val Loss=1.3151\n",
      "0.5294117647058824\n",
      "Epoch 2: Train Loss=1.5738, Val Loss=1.8779\n",
      "0.5294117647058824\n",
      "Epoch 3: Train Loss=1.5742, Val Loss=1.7885\n",
      "0.5294117647058824\n",
      "Epoch 4: Train Loss=1.5740, Val Loss=1.5938\n",
      "Early stopping at epoch 4\n",
      "0.25\n",
      "Epoch 1: Train Loss=1.5735, Val Loss=2.1191\n",
      "0.25\n",
      "Epoch 2: Train Loss=1.5738, Val Loss=1.3085\n",
      "0.25\n",
      "Epoch 3: Train Loss=1.5741, Val Loss=1.6327\n",
      "0.25\n",
      "Epoch 4: Train Loss=1.5745, Val Loss=1.4512\n",
      "0.25\n",
      "Epoch 5: Train Loss=1.5746, Val Loss=1.6121\n",
      "Early stopping at epoch 5\n",
      "0.5294117647058824\n",
      "Epoch 1: Train Loss=1.5740, Val Loss=2.1648\n",
      "0.5294117647058824\n",
      "Epoch 2: Train Loss=1.5747, Val Loss=1.6734\n",
      "0.5294117647058824\n",
      "Epoch 3: Train Loss=1.5733, Val Loss=1.5970\n",
      "0.5294117647058824\n",
      "Epoch 4: Train Loss=1.5742, Val Loss=1.6509\n",
      "0.5294117647058824\n",
      "Epoch 5: Train Loss=1.5739, Val Loss=1.4518\n",
      "0.5294117647058824\n",
      "Epoch 6: Train Loss=1.5738, Val Loss=1.4905\n",
      "0.5294117647058824\n",
      "Epoch 7: Train Loss=1.5742, Val Loss=1.3086\n",
      "0.5294117647058824\n",
      "Epoch 8: Train Loss=1.5747, Val Loss=1.4824\n",
      "0.5294117647058824\n",
      "Epoch 9: Train Loss=1.5747, Val Loss=1.4140\n",
      "0.5294117647058824\n",
      "Epoch 10: Train Loss=1.5733, Val Loss=1.4510\n",
      "Early stopping at epoch 10\n",
      "0.5294117647058824\n",
      "Epoch 1: Train Loss=1.5743, Val Loss=1.4512\n",
      "0.5294117647058824\n",
      "Epoch 2: Train Loss=1.5737, Val Loss=1.6297\n",
      "0.5294117647058824\n",
      "Epoch 3: Train Loss=1.5741, Val Loss=1.8773\n",
      "0.5294117647058824\n",
      "Epoch 4: Train Loss=1.5746, Val Loss=1.5943\n",
      "Early stopping at epoch 4\n",
      "0.25\n",
      "Epoch 1: Train Loss=1.5738, Val Loss=1.5939\n",
      "0.25\n",
      "Epoch 2: Train Loss=1.5736, Val Loss=1.8798\n",
      "0.25\n",
      "Epoch 3: Train Loss=1.5736, Val Loss=1.4511\n",
      "0.25\n",
      "Epoch 4: Train Loss=1.5745, Val Loss=1.7365\n",
      "0.25\n",
      "Epoch 5: Train Loss=1.5736, Val Loss=1.6859\n",
      "0.25\n",
      "Epoch 6: Train Loss=1.5738, Val Loss=1.5939\n",
      "Early stopping at epoch 6\n",
      "0.5294117647058824\n",
      "Epoch 1: Train Loss=1.5739, Val Loss=1.5611\n",
      "0.5294117647058824\n",
      "Epoch 2: Train Loss=1.5742, Val Loss=1.3801\n",
      "0.5294117647058824\n",
      "Epoch 3: Train Loss=1.5742, Val Loss=1.4512\n",
      "0.5294117647058824\n",
      "Epoch 4: Train Loss=1.5739, Val Loss=1.5943\n",
      "0.5294117647058824\n",
      "Epoch 5: Train Loss=1.5743, Val Loss=1.7481\n",
      "Early stopping at epoch 5\n",
      "0.5294117647058824\n",
      "Epoch 1: Train Loss=1.5738, Val Loss=1.5812\n",
      "0.5294117647058824\n",
      "Epoch 2: Train Loss=1.5747, Val Loss=2.0217\n",
      "0.5294117647058824\n",
      "Epoch 3: Train Loss=1.5747, Val Loss=1.5940\n",
      "0.5294117647058824\n",
      "Epoch 4: Train Loss=1.5745, Val Loss=1.3088\n",
      "0.5294117647058824\n",
      "Epoch 5: Train Loss=1.5738, Val Loss=1.4793\n",
      "0.5294117647058824\n",
      "Epoch 6: Train Loss=1.5741, Val Loss=1.5951\n",
      "0.5294117647058824\n",
      "Epoch 7: Train Loss=1.5741, Val Loss=1.5623\n",
      "Early stopping at epoch 7\n",
      "0.25\n",
      "Epoch 1: Train Loss=1.5743, Val Loss=1.5939\n",
      "0.25\n",
      "Epoch 2: Train Loss=1.5744, Val Loss=1.5939\n",
      "0.25\n",
      "Epoch 3: Train Loss=1.5733, Val Loss=1.4375\n",
      "0.25\n",
      "Epoch 4: Train Loss=1.5739, Val Loss=1.5946\n",
      "0.25\n",
      "Epoch 5: Train Loss=1.5741, Val Loss=1.5942\n",
      "0.25\n",
      "Epoch 6: Train Loss=1.5741, Val Loss=1.4780\n",
      "Early stopping at epoch 6\n",
      "0.5294117647058824\n",
      "Epoch 1: Train Loss=1.5731, Val Loss=1.5983\n",
      "0.5294117647058824\n",
      "Epoch 2: Train Loss=1.5742, Val Loss=1.5943\n",
      "0.5294117647058824\n",
      "Epoch 3: Train Loss=1.5735, Val Loss=1.6147\n",
      "0.5294117647058824\n",
      "Epoch 4: Train Loss=1.5743, Val Loss=1.5941\n",
      "0.5294117647058824\n",
      "Epoch 5: Train Loss=1.5747, Val Loss=1.4514\n",
      "0.5294117647058824\n",
      "Epoch 6: Train Loss=1.5738, Val Loss=1.5744\n",
      "0.5294117647058824\n",
      "Epoch 7: Train Loss=1.5750, Val Loss=2.0210\n",
      "0.5294117647058824\n",
      "Epoch 8: Train Loss=1.5737, Val Loss=1.5944\n",
      "Early stopping at epoch 8\n",
      "0.5294117647058824\n",
      "Epoch 1: Train Loss=1.5747, Val Loss=1.4514\n",
      "0.5294117647058824\n",
      "Epoch 2: Train Loss=1.5741, Val Loss=1.5658\n",
      "0.5294117647058824\n",
      "Epoch 3: Train Loss=1.5739, Val Loss=1.6272\n",
      "0.5294117647058824\n",
      "Epoch 4: Train Loss=1.5746, Val Loss=1.5935\n",
      "Early stopping at epoch 4\n",
      "Best Precision: 0.5294117647058824\n",
      "Best Model: CovertypeClassification(\n",
      "  (layer1): Linear(in_features=54, out_features=128, bias=True)\n",
      "  (act1): ReLU()\n",
      "  (drop1): Dropout(p=0.2, inplace=False)\n",
      "  (layer2): Linear(in_features=128, out_features=128, bias=True)\n",
      "  (act2): ReLU()\n",
      "  (drop2): Dropout(p=0.2, inplace=False)\n",
      "  (layer3): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (act3): ReLU()\n",
      "  (output): Linear(in_features=64, out_features=7, bias=True)\n",
      "  (softmax): Softmax(dim=1)\n",
      ")\n",
      "Best learning rate: 0.001\n",
      "Best size of hidden layers: 64\n",
      "Best batch size: CovertypeClassification(\n",
      "  (layer1): Linear(in_features=54, out_features=128, bias=True)\n",
      "  (act1): ReLU()\n",
      "  (drop1): Dropout(p=0.2, inplace=False)\n",
      "  (layer2): Linear(in_features=128, out_features=128, bias=True)\n",
      "  (act2): ReLU()\n",
      "  (drop2): Dropout(p=0.2, inplace=False)\n",
      "  (layer3): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (act3): ReLU()\n",
      "  (output): Linear(in_features=64, out_features=7, bias=True)\n",
      "  (softmax): Softmax(dim=1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 30\n",
    "\n",
    "\n",
    "for lr in learning_rate:\n",
    "    for size in size_of_hidden_layers:\n",
    "        for batch_size in batch_sizes:\n",
    "            model = CovertypeClassification(54, size, 7)\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "            \n",
    "            train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "            val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "            \n",
    "            for epoch in range(EPOCHS):\n",
    "                train_loss = 0.0\n",
    "                val_loss = 0.0\n",
    "                net.train()\n",
    "                for x, y in train_loader:\n",
    "                    x = x.to(device)\n",
    "                    y = y.to(device)\n",
    "                    preds = net(x)\n",
    "                    loss = loss_fn(preds, y)\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    train_loss += loss.item()\n",
    "        \n",
    "                net.eval()\n",
    "                with torch.no_grad():\n",
    "                    for x_val, y_val in val_loader:\n",
    "                        preds_val = net(x_val)\n",
    "                        loss_val = loss_fn(preds_val, y_val)\n",
    "                        val_loss += loss.item() \n",
    "\n",
    "                precision = precision_score(y_val, torch.argmax(preds_val, dim = 1), average = 'weighted', zero_division = 0)\n",
    "                print(precision)\n",
    "                \n",
    "                if precision > best_precision:\n",
    "                    best_precision = precision\n",
    "                    best_model = net\n",
    "                    best_lr = lr\n",
    "                    best_size = size\n",
    "                    best_batch_size = batch_size\n",
    "                    \n",
    "                train_loss /= len(train_loader)\n",
    "                val_loss /= len(val_loader)\n",
    "\n",
    "                print(f\"Epoch {epoch+1}: Train Loss={train_loss:.4f}, Val Loss={val_loss:.4f}\")\n",
    "                \n",
    "                if val_loss < best_loss:\n",
    "                    best_loss = val_loss\n",
    "                    best_epoch = epoch\n",
    "                    tolerance = 0\n",
    "                else:\n",
    "                    tolerance += 1\n",
    "            \n",
    "                if tolerance > max_tolerance:\n",
    "                    print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                    best_loss = float('inf')\n",
    "                    best_epoch = 0\n",
    "                    tolerance = 0\n",
    "                    max_tolerance = 2\n",
    "                    break\n",
    "\n",
    "\n",
    "\n",
    "print(\"Best Precision:\", best_precision)\n",
    "print(\"Best Model:\", best_model)   \n",
    "print(\"Best learning rate:\", best_lr) \n",
    "print(\"Best size of hidden layers:\", best_size) \n",
    "print(\"Best batch size:\", best_model) \n",
    "\n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
